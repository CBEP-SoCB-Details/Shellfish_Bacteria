---
title: "Shellfish Sanitation Program Data Assembly"
output: html_notebook
---

# Install Libraries
```{r}
library(readr)
library(readxl)
library(tidyverse)
```

# Load Data
First we create a string that gets us to our sibling folder, then load the data.  Unfortunately, the data file formats are not entirely compatible, so we need to handle this carefully.
```{r}
sibfldnm <- 'Original Data'
parent <- dirname(getwd())
sibling <- file.path(parent,sibfldnm)

fl1<- "2016 p90 for CBEP.xlsx"
fl2 <- "2017 p90 for CBEP.xlsx"
fl3 <- "MaineDMR_Public_Health__2018_P90_Scores.csv"
fl4 <- "Casco Bay WQ 15.19.csv"

path <- file.path(sibling, fl1)
p90.data.2018 <- read_excel(path) %>%
  select(-OBJECTID) %>%
  select(-Min_Date) %>%
  mutate(Year = 2016)

path <- file.path(sibling, fl2)
tmp1 <- read_excel(path) %>%
  select(-OBJECTID) %>%
  select(-Min_Date) %>%
  mutate(Year = 2017)

path <- file.path(sibling, fl3)
tmp2 <- read_csv(path) %>%
  select(-OBJECTID) %>%
  select(-X,-Y) %>%
  select(-GlobalID) %>%
  rename(Count=Count_, MAX=MAX_) %>%
  mutate(Year = 2018)
```

# P90 Summary Data
```{r}
#Check Consistency of mnames and order
names(p90.data.2018)
names(tmp1)
names(tmp2)
```

# The summary data is not what it appears
The data released by DMR is a summary of findings over the prior 30 observations, which may occur over multiple years.  In practice, that means the 2016, 2017, and 2018 data releases are not independent, since only a handful of samples are collected at any given location.

To show this graphically:

```{r}
p90.data <- rbind(p90.data.2018,tmp1, tmp2) %>%
  mutate_at(c('Station','Class', 'Grow_Area'), factor)
rm(tmp1, tmp2)
```
```{r}
p90.data %>% 
  filter (Year >2016) %>%
  select(Station, Year, GM) %>%
  pivot_wider(names_from=Year, names_prefix='Yr',
              values_from=GM) %>%
  ggplot(aes(Yr2017, Yr2018)) + geom_point() +
  geom_abline(slope = 1, intercept=0, color = 'red')
```
So, there's a very close correlation between the geometric means reported in subsequent years.

```{r}
p90.data %>% 
  filter (Year >2016) %>%
  select(Station, Year, P90) %>%
  pivot_wider(names_from=Year, names_prefix='Yr',
              values_from=P90) %>%
  ggplot(aes(Yr2017, Yr2018)) + geom_point() +
  geom_abline(slope = 1, intercept=0, color = 'red')
```
The same is true for the p90 value here.

If you prefer correlations:
```{r}
tmp <- p90.data %>% 
  filter (Year >2016) %>%
  select(Station, Year, P90) %>%
  pivot_wider(names_from=Year, names_prefix='Yr',
              values_from=P90)
cor(tmp$Yr2017, tmp$Yr2018, use = 'pairwise')
cor(log(tmp$Yr2017), log(tmp$Yr2018), use = 'pairwise')
cor(tmp$Yr2017, tmp$Yr2018, use = 'pairwise', method = 'spearman')
```

## Check Data Structure
Are statistics based on thirty observations or not.
```{r}
# Total Sample Size
with(p90.data,length(Count ))
# How many results are based on 30 observations?
with(p90.data,sum(Count > 30, nm.rm=TRUE))
with(p90.data,sum(Count < 30, nm.rm=TRUE))
#What was the lowest number of samples that underly a value?
with(p90.data,min(Count, nm.rm=TRUE))
# HOw many missing values?
with(p90.data,sum(is.na(Count)))
```
So, most data is based on 30 observations, as expected.
## Data Consistency 
Checking to see whther we have mismatches between reported number of samples and checking whether all samples are useing the same threshold values for whether stations can be considered approved or restricted.
```{r}
with(p90.data,sum(Count != MFCount, nm.rm=TRUE))
with(p90.data, sum(Appd_Std != 31, nm.rm=TRUE))
with(p90.data, sum(Restr_Std != 163, nm.rm=TRUE))
```
So, things are almost all consistent.

There is no advantage to keeping all three year's data -- except that it may provide a convenient list of regularly sampled locations.  The primary value for us is the convenient sumamries DMR produces annually -- which may be good enough for some graphic displays, if all we want to show are the geometric means or the p90 values.

# Extract complete list of Locations / Stations
```{r}
p90.locs.data <- p90.data %>%
  select(Station, Lat_DD, Long_DD) %>%
  group_by(Station) %>%
  summarize(Lat = first(Lat_DD), Long = first(Long_DD))
```
## Export Locations Data As CSV Files
```{r}
cn <- colnames(p90.locs.data)
write.table(p90.locs.data, 'Shellfish p90 Locations.csv',
            sep = ',', na='', row.names = FALSE, col.names = cn)
```

```{r}
rm(p90.data)
```

#Raw Observations 2015 through 2018
After realizing the format of the p90 data, we  requested raw data from DMR.  The primary reason for requesting these data is that DMR does not use available information eficiently to evaluate very low bacteria numbers, and does not look at frequency of elevated samples -- both of which may be important for understanding what is going on.

DMR also collects temperature and salinity data, which is not reported in the summary files.

## Load Data
Most of this code is to skip data columns that were used for data QA/QC, or to ensure dates and times are properly interpreted.
```{r}
fn<- "Casco Bay WQ 15.19.csv"
path <- file.path(sibling, fn)
raw.data <- read_csv(path, 
    col_types = cols(
        ADVERSITY = col_skip(),  
        CATCH_COMMENTS = col_skip(), 
        CATCH_SEQ_NO = col_skip(),
        CATCH_UPDATE_DATE = col_skip(), 
        CATCH_UPDATE_USER = col_skip(),
        COL_METHOD = col_skip(), 
        DMR_CATCH_IDENTIFIER = col_skip(), 
        DMR_EFFORT_IDENTIFIER = col_skip(), 
        DMR_SAMPLE_IDENTIFIER = col_skip(), 
        EFFORT_COMMENTS = col_skip(),
        EFFORT_SEQ_NO = col_skip(), 
        EFFORT_START_DATE = col_date(format = "%Y-%m-%d"), 
        EFFORT_START_TIME = col_time(format = "%H:%M"), 
        EFFORT_UPDATE_DATE = col_skip(), 
        EFFORT_UPDATE_USER = col_skip(), 
        EXAM_DATE_TIME = col_skip(),
        INITIATED_BY = col_skip(), 
        INITIATED_DATE = col_skip(),
        LAB = col_skip(), 
        LAT_DD = col_skip(),
        LON_DD = col_skip(), 
        MISSED_STATION_CODE = col_skip(),
        ROW_NUMBER = col_skip(),
        SAMPLE_COMMENTS = col_skip(),
        SAMPLE_METHOD = col_skip(),
        SAMPLE_SEQ_NO = col_skip(), 
        SAMPLE_UPDATE_DATE = col_skip(), 
        SAMPLE_UPDATE_USER = col_skip(),
        STRATEGY = col_skip(),  
        TRIP_SEQ_NO = col_skip(),
        X = col_skip(),
        X1 = col_skip())) %>%
  mutate_at(c('LOCATION_ID', 'GROWING_AREA', 'OPEN_CLOSED_FLAG',
              'WIND_DIRECTION','TIDE_STAGE',
              'CURRENT_CLASSIFICATION_CODE',
              'CATEGORY'), factor) %>%
  mutate(YEAR = as.numeric(format(EFFORT_START_DATE, "%Y")))
```
## Review Status of Data
```{r}
with(raw.data, length(levels(LOCATION_ID)))
with(raw.data, levels(GROWING_AREA))
with(raw.data, levels(OPEN_CLOSED_FLAG))
with(raw.data, sum(OPEN_CLOSED_FLAG=='X', na.rm=TRUE))
with(raw.data, xtabs(~OPEN_CLOSED_FLAG+factor(YEAR)))
```
So, the "X" flag was not used prior to 2018, and mostly in 2019.  Is that because decisions to close the flat are not made immediately?  Perhaps this factor is not useful for us.
```{r}
with(raw.data, levels(WIND_DIRECTION))
with(raw.data, xtabs(~WIND_DIRECTION+factor(YEAR)))
```
So, for some reason, "NW" was not used prior to 2019.  NNE was only used in 2016.  Many fewer N observations after 2017. MAny fewer SE observations in 2016.  These data look inconsistent over time, so may not be all that useful foir analysis.
```{r}
with(raw.data, levels(TIDE_STAGE))
with(raw.data, levels(CURRENT_CLASSIFICATION_CODE))
with(raw.data, levels(CATEGORY))
```

```{r}
with(raw.data, xtabs(~CATEGORY+CURRENT_CLASSIFICATION_CODE))
with(raw.data, xtabs(~OPEN_CLOSED_FLAG+CURRENT_CLASSIFICATION_CODE))
```
Note that there's a one to one match between category -= Z and Current Classification = X.  Is this a change in data categories?  Also note that Category = I was only used once.

Current classification liooks like it may be useful, but I'm not sure what to do with Cuurent Classification =, which is almost al 2019 data, pehpas because status has not been finalized yet?

## Convert NNE wind direction to N
NNE was used for only 10 observations in 2016, and its use is inconsistent with recording wind direction  in eight directions.
```{r}
raw.data <- raw.data %>%
  mutate(WIND_DIRECTION = factor(if_else(WIND_DIRECTION == 'NNE', 'N',
                                         as.character(WIND_DIRECTION))))
with(raw.data, xtabs(~WIND_DIRECTION+factor(YEAR)))
```
## Convert Order of Factors
Since these are factors, and not ordered factors, this is merely to make it a bit easier to look at results.  It also has the effect of changing the base category for contrasts.  Note that this will not affect ordering of factors when data is loaded in from CSV files, so this code (or something very like it) will have to be run in the data analysis scripts too.
```{r}
raw.data <- raw.data %>%
  mutate(WIND_DIRECTION =factor(WIND_DIRECTION,
                                levels = c("CL", "N", "NE",
                                           "E","SE", "S","SW",
                                           "W", "NW"))) %>%
  mutate(CURRENT_CLASSIFICATION_CODE = factor(CURRENT_CLASSIFICATION_CODE,
                                              levels = c( 'A', 'CA', 'CR',
                                                          'R', 'P', 'X' ))) %>%
  mutate(TIDE_STAGE = factor(TIDE_STAGE, levels = c("L", "LF", "F", "HF",
                                                    "H", "HE", "E", "LE")))
```

## Simplify Column Names
Data names from DMR are long, in all caps, and awkward, so we want to simplify.  While we are at it, we want names to more or less match what we found in the p90 files.
```{r}
names(raw.data)
```

```{r}
raw.data <- raw.data %>%
  select(-FLOOD, -DELIVERY_TEMP_C) %>%
  rename(SDate = EFFORT_START_DATE,
         STime = EFFORT_START_TIME,
         SDateTime = START_DATE_TIME,
         Station =  LOCATION_ID,
         GROW_AREA =  GROWING_AREA,
         OpenClosed = OPEN_CLOSED_FLAG,
         WDIR = WIND_DIRECTION,
         Tide =  TIDE_STAGE,
         Class = CURRENT_CLASSIFICATION_CODE,
         Temp = TEMP_C,
         Sal = SALINITY_PCT,
         ColiScore = COL_SCORE,    # This value uses inapporpriate substitution based on right censored values.
         RawColi = RAW_COL_SCORE)
```
# Deal With Censoring
This data has both left and right censoring, although right censoring is relatively uncommon.  I am unaware of a library in R that handles doubly censored data with grace, however it would not be difficult to develop my own maximum likelihood methods.
```{r}
the.data <- raw.data %>%
  #Identify censored data
  mutate(LCFlag = substr(RawColi,1,1)=='<') %>%
  mutate(RCFlag = substr(RawColi,1,1)=='>') %>%
  mutate(ColiVal = if_else(LCFlag,
                           substr(RawColi,2,nchar(RawColi)),
                           RawColi )) %>%
  mutate(ColiVal = if_else(RCFlag,
                           substr(RawColi,2,nchar(RawColi)),
                           ColiVal)) %>%
  mutate(ColiVal = as.numeric(ColiVal))
```

# Data export
```{r}
cn <- colnames(the.data)
write.table(the.data, 'Shellfish data 2015 2018.csv', sep = ',',
            na='', row.names = FALSE, col.names = cn)
```

# Final Data Cleanup
We may want to directly load the data produces in this Notebook for use in analysis, rather than importing it from the CSV file, so we might as well clean things up.
```{r}
rm(cn, fn, fl1, fl2, fl3, fl4, path, parent, sibfldnm, sibling)
```

