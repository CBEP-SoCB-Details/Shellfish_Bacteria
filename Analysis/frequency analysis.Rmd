---
title: "Frequency of Exceedences Analysis"
author: "Curtis C. Bohlen, Casco Bay Estuary Partnership."
date: "11/14/2020"
output:
  github_document:
    toc: true
    fig_width: 7
    fig_height: 5
---
<img
    src="https://www.cascobayestuary.org/wp-content/uploads/2014/04/logo_sm.jpg"
    style="position:absolute;top:10px;right:50px;" />

# Introduction
The bacteria data is highly skewed. We have found no ready way to estimate a
gemometric mean for these data in a robust way.  The data appears distributed
close to a Pareto distribution, which is highly skewed, with a heavy right tail.
This distribution is likely to be difficult to model, so we can not readily
estimate parameters, moments and other summary statistics.

One possiblity would be to run some sort of (generalized) linear model on the
log of data, calculate means and medians, and back transform. I hope to explore
both linear model approaches in another R Notebook.

Here we take another approach, which is to focus on binomial, quasi-binomial,
and multinomial proportional odds models to estimate probabilities of exceeding
different regulatory thresholds.

## Growing Area Classification Standards

Growing Area Classification | Activity Allowed              |	Geometric mean FC/100ml	| 90th Percentile (P90) FC/100ml
----------------------------|-------------------------------|-------------------------|-------------------------------
Approved	               | Harvesting allowed	                  | ≤ 14	              | ≤ 31
Conditionally Approved	 | Harvesting allowed except during specified conditions | ≤ 14 in open status	| ≤ 31 in open status
Restricted	             | Depuration harvesting or relay only	| ≤ 88 and >15	      | ≤ 163 and > 31
Conditionally Restricted | Depuration harvesting or relay allowed except during specified conditions	| ≤ 88 in open status	| ≤ 163 in open status
Prohibited	             | Aquaculture seed production only	    | >88	                |>163

So, critical levels for Geometric Mean include:

*  $GM <= 14$: Approved, or Open status at Conditionally Approved sites

*  $GM <= 88$: Depuration harvesting or Relay Only

*  $GM > 88$ : Prohibited

And for the p90:

*  $P90 < 31$    Approved or Open status at Conditionally Approved

*  $P90 <= 163$  Depuration harvesting or Relay Only

*  $P90 > 163$   Prohibited

# Load Libraries
```{r}
library(MASS)   # Load before tidyverse because it has a select() function
library(tidyverse)

library(readr)
library(GGally)

library(mgcv)   # For GAMs and GAMMs; used her for seasonal smoothers
library(emmeans)
```

# Load Data
```{r}
sibfldnm <- 'Derived_Data'
parent <- dirname(getwd())
sibling <- file.path(parent,sibfldnm)
fn = 'Shellfish data 2015 2018.csv'
path <- file.path(sibling, fn)
coli_data <- read_csv(path)

```

## Convert to Factors
```{r}
coli_data <- coli_data %>%
  mutate(across(Station:WDIR, factor)) %>%
  mutate(Class = factor(Class,levels = c( 'A', 'CA', 'CR',
                                         'R', 'P', 'X' ))) %>%
  mutate(Tide = factor(Tide, levels = c("L", "LF", "F", "HF",
                                        "H", "HE", "E", "LE"))) %>%
  mutate(DOY = as.numeric(format(SDate, format = '%j')),
         month = as.numeric(format(SDate, format = '%m'))) %>%
  mutate(month = factor(month, labels = month.abb)) %>%
  rename_with(tolower)
```

## Calculate Indicator Variables
Here we calculate variables that indicate whether each sample exceeds our four
thresholds.  Then they are combined to produce three multinomial ordered
factors.

So, the key thresholds are these:
```{r}
coli_limits <- list(open = 0,   gmrelay=14,    p90relay=31, 
                    gmclosed=88, p90closed=163, high= 5000)

freq_data<- coli_data %>%
  filter(! is.na(colival)) %>%
  mutate(gm_open   = colival <= 14,
         gm_relay  = colival <= 81,
         p90_open  = colival <= 31,
         p90_relay = colival <= 163) %>%
  mutate(all_lvls = cut(colival, coli_limits,
                    labels = c('open', 'gm_relay', 'p90_relay', 
                               'gm_closed', 'p90_closed'),
                    ordered_result = TRUE)) %>%
  mutate(p90_lvls = cut(colival, coli_limits[c(1,3,5,6)], 
                    labels = c('p90open', 'p90relay', 'p90closed'),
                    ordered_result = TRUE)) %>%
  mutate(gm_lvls = cut(colival, coli_limits[c(1,2,4,6)], 
                      labels = c('gmopen', 'gmrelay', 'gmclosed'),
                      ordered_result = TRUE))
```

```{r}
freq_data <- freq_data %>% 
  select(-coliscore, -rawcoli, -lcflag, -rcflag, -colival)
```

```{r}
freq_data %>% select(month, all_lvls) %>%
  mutate(month=factor(month)) %>%
  ggpairs(aes(month, all_lvls)) 
  
```

1. Most observations are from summer months
2. Most observations are lower than all our cut points
3. Probability of exceedences appears slightly higher in summer months.

```{r fig.width = 5, fig.height = 3}
freq_data %>%
  count(month, all_lvls) %>%
  ggplot(aes(month, n, fill=all_lvls)) +
   geom_bar(stat = "identity", position='fill') +
  theme_minimal()
```

# Modeling
We'll explore three different modeling strategies:
1. A binomial GLM for exceedences of the median and p90 thresholds.
2. A multinomial proportional odds model using the polr function from MASS
3. A mutinomial model using VGLM

It's clear we need to account for different sampling histories to provide robust
comparable estimates of site probabilities.

# Binomial GLM
## Preliminaries
We want to be able to compare the results of modeling to observed relative
frequencies of meeting or exceeding standards, so we want a simple data frame
containing those probabilities.  We structured our binomial observations
with TRUE = open, so these are probabilities of meeting standards.

```{r}
rawprobs <- freq_data %>%
  group_by(station) %>%
  summarize(p_gm_open  = sum(gm_open/n()),
            p_p90_open = sum(p90_open/n()))
ggplot(rawprobs, aes(p_gm_open, p_p90_open)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  geom_hline(yintercept = 0.1, lty = 2) +  # p90 violation would be below this
  coord_equal()
```
Which looks pretty good. No Site has a relative frequency of exceeding the p90
standard greater than 10%.

#Cleanup
We remove unnecessary data frames to free memory for calculations.
```{r}
rm(coli_data, coli_limits)
```

# Initial Model

## Utility Functions
Remember the coefficients of a binomial GLM are actually the logit of the p
robabilities we are interested in, so a couple of utility functions come in 
handy. These are probably not the most numerically stable versions of these
functions for extreme values, but they work.
```{r}
logit <- function(p) {log(p)-log(1-p)}
inv_logit <- function(x) {1/(1+exp(-x))}
```

```{r}
system.time(gm_glm1 <- glm(gm_open~station +month + 0,
             data = freq_data,
             family=binomial(link=logit)))
```


```{r}
anova(gm_glm1, test='LRT')
```

As expected, months are highly statistically significant -- a finding of little
value here.  We are interested in the coefficients for the Stations, which are
effectively adjusted for any deviations in sampling frequency by month.

The residual deviance here is quite small for more than 9000 df....



```{r}
res<-coef(gm_glm1)[1:239]
pres<- inv_logit(res)

rawprobs %>%
  mutate(predicted = pres) %>%
  ggplot(aes(p_gm_open, predicted) ) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  coord_equal()
```
So, once adjusted for monthly sampling differences, the remainder of the
probabilities are only about one third as high as what we observe in the "real"
data.... Why?

Because these values are based on the default month, which is.....
January, as can be confirmed by looking at the model summary or list of
coefficients.

We have a couple of different approaches we can take to deal with this -- the
first is to relevel the months, picking a different reference month. The second
is to use estimated marginal means, which are the equivalent in R of what are
called least squares means in SAS.  The emmeans package replaces the older
lsmeans package.

I prefer the second approach.

```{r}
mmrg <- ref_grid(gm_glm1)  # Marginal Means Reference Grid
mm <- emmeans(gm_glm1, "station")
mms <- summary(mm)  # Summary has class dataframe, making access and display easier.
head(mms, 5)  
```
```{r}
pred <- inv_logit(mms$emmean)

rawprobs %>%
  mutate(predicted = pred) %>%
  ggplot(aes(p_gm_open, predicted) ) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  coord_equal()

```
Now that looks pretty good.

As expected, the predicted probabilities are lower than the observed
frequencies, because the real data "oversamples" the higher risk summer months.

Note that what we have here are the probabilities that an individual observation
meets the Geometric Mean thresholds.  All the probabilities are above 50%,
suggesting all sample locations should be open, and they are not. It would be
nice to color these by some of the predictor variables....  but for that we need
to extract more predictors on a station by station basis.

Note also that the high correlation -- eyeballing that graphic suggests the
correlation is over 0.9 -- means there's only a little advantage to adjusting
sites for their seasonal sampling history.

Can I generate estimated marginal means focusing on summer months?
Or can I drop the 2019 data, which is unequal monthly sampling from historical 
data, and then use the actual marginal means.










